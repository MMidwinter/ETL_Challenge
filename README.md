<h1> Project Title: Books Books Books! </h1>

Team Members: Melissa Lowe, Michael Friesen, Matt Midwinter

Project Description / Overview: Uploading 2 different databases to get the detailed information on books and a list of user ratings for those books.


Datasets (be specific!): 

We have found two data sets on Kaggle located below.

Goodbooks-10k: https://www.kaggle.com/zygmunt/goodbooks-10k?select=books.csv

bookcrossing: https://www.kaggle.com/ruchi798/bookcrossing-dataset




Rough Breakdown of Tasks:
* E: We will be extracting data sets to get more robust ratings for our good books data that we can later use to analyze the good books data set.
* T: We will need to transform our data to make sure all columns are named the same and drop any columns that would potentially be used in our analysis. 
* L: We will load 2-4 tables into our database with keys that reference one another. Our database will be using Postgres


<h1>Process notes</h1>

Clone this repo to your desktop; it will contain the three necessary source csvs to produce the database.

Launch a jupyter notebook from the repo and open ETL_workbook.
Notes on how csvs were cleaned and issues resolved in dataframes:
* GoodBooks csv 
  * was not encoded in UTF-8 and had to be specified to be encoded in Latin1. 
  * This document was also separated by semicolons vs commas
  * There were additional errors in rows, so additional code was included to skip those rows when assembling the cv
* BookCrossing Books csv
  * Rating counts were dropped as that data can be generated by joining the book table and ratings table from the database.  Additional space can be conserved by excluding those calculations and allowing those interacting with the database to form their own analysis.
* Book_crossing_data
  * NaNs dropped before setting the index as there was an absent publication year for BossyPants by Tina Fey. We also dropped NaNs from good_books_ratings to avoid duplicate ISBN issue in our index and primary key column.
* Book_crossing_ratings
  * The dataframe presented a duplicate user_id and book_id combination, so we returned to the dataframe only keep the first appearance of that combination so that the user_id column remained a unique identifier.
* Good_books_ratings
  * Dropped NaNs and duplicates present in the isbn column before setting as the index
* Column names were conformed to the match across dataframes and tables
* Each dataframe has the isbn set as the index, save for the good_books_rating dataframe where the user_id is the unique identifier used as the index. These indexes are used as the primary keys in the sql tables

After running the code to clean the csvs, open PGAdmin to access Postgress.
Log in and create a database called bookrating_db.
Create three tables as described below using the included table_schema.sql:
* Good_books_ratings
    * Isbn TEXT primary key
    * Book_rating INT
* book_crossing_ratings
    * Usesr_id INT PRIMARY KEY
    * Book_id INT PRIMARY KEY
    * Book_rating INT
* Book_crossing_data
    * Isbn TEXT PRIMARY KEY
    * Book_id INT
    * Authors TEXT
    * Original_publication_year INT
    * Original_title TEXT


This database is intended to be built in SQL using Postgres and PGAdmin.  Please note that the code following the cleaning of the csv files includes a username and password requirement to communicate with Postgres,  Before running those cells, be sure to enter your own username and password before running the code.

Loading the data into the dataframe
* Book_crossing_ratings is a large dataset and may take longer than the other two to run when uploading data to the database.
